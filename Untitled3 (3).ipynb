{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi  # Если выдаёт ошибку — доступ закрыт"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVX6Biz7Wic6",
        "outputId": "d30731bf-68bd-4c69-c02b-ab9683edb67b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-YngRGHBB0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795b12d3-e3b8-457c-eedc-aba1a13a245a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch torchvision openssh-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_num_threads(os.cpu_count())"
      ],
      "metadata": {
        "id": "ihEORs_qg3ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет -- 1. Загрузка рубленного датасета с диска ---> 2. Предобработка и Токенизация ---> 3. Создание DataLoader"
      ],
      "metadata": {
        "id": "U-qRGWQp7dpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ЗАГРУЗКА ДАННЫХ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "all_data = []\n",
        "for i in range(1, 51):\n",
        "    file_path = f\"/content/drive/MyDrive/dataset/subtitles_text_{i}.csv\"\n",
        "    try:\n",
        "        chunk = pd.read_csv(file_path)\n",
        "        chunk['en'] = chunk['en'].astype(str).str.strip()\n",
        "        chunk['ru'] = chunk['ru'].astype(str).str.strip()\n",
        "        chunk = chunk[(chunk['en'] != '') & (chunk['ru'] != '')]\n",
        "        all_data.append(chunk)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке {file_path}: {str(e)}\")\n",
        "\n",
        "if not all_data:\n",
        "    raise ValueError(\"Не загружено ни одного файла!\")\n",
        "\n",
        "full_data = pd.concat(all_data, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weSn32EEeVIZ",
        "outputId": "72da6770-343b-4899-d42a-6504fa8e60fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_data.head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKwHsbKEDekc",
        "outputId": "d62b046b-9481-48e5-9a57-eaad9401d506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of                                                          en  \\\n",
            "0                                      - Get away from her!   \n",
            "1                                      - Get away from her!   \n",
            "2                                     She can't leave here.   \n",
            "3                              - Her heart stopped beating.   \n",
            "4                                        - She can't leave.   \n",
            "...                                                     ...   \n",
            "24999995   Who's gonna remember them if we don't film them?   \n",
            "24999996  The battle of Grunwald was never filmed, but s...   \n",
            "24999997                   - But you'd watch it? - I would.   \n",
            "24999998                           They're playing Mazurek.   \n",
            "24999999                         Do you know what it means?   \n",
            "\n",
            "                                                         ru  \n",
            "0                                          Отойдите от неё.  \n",
            "1                                        - Отойдите от неё.  \n",
            "2                                        Её нельзя увозить.  \n",
            "3                                   У неё остановка сердца.  \n",
            "4                                         Сердце не бьётся.  \n",
            "...                                                     ...  \n",
            "24999995                       Витусь, ну это же повстанцы.  \n",
            "24999996  Кто их запомнит, если мы их не снимем? Битву п...  \n",
            "24999997                           - Но ты бы посмотрел ее?  \n",
            "24999998                                    Витек, слышишь?  \n",
            "24999999                                  Мазурку включили.  \n",
            "\n",
            "[25000000 rows x 2 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. СОЗДАНИЕ СЛОВАРЕЙ\n",
        "def build_vocab(texts, max_vocab_size=50000, min_word_count=3):\n",
        "    \"\"\"\n",
        "    Создает словарь с фильтрацией редких слов\n",
        "\n",
        "    :param texts: входные тексты\n",
        "    :param max_vocab_size: максимальный размер словаря\n",
        "    :param min_word_count: минимальная частота слова для включения\n",
        "    :return: словарь {слово: индекс}\n",
        "    \"\"\"\n",
        "    # Специальные токены\n",
        "    vocab = {\n",
        "        \"<pad>\": 0,\n",
        "        \"<unk>\": 1,\n",
        "        \"<sos>\": 2,\n",
        "        \"<eos>\": 3\n",
        "    }\n",
        "\n",
        "    # Подсчет частот слов\n",
        "    word_counts = defaultdict(int)\n",
        "    total_words = 0\n",
        "\n",
        "    for text in texts:\n",
        "        text = str(text).strip()\n",
        "        if not text or text.lower() == 'nan':\n",
        "            continue\n",
        "        for word in text.split():\n",
        "            word_counts[word] += 1\n",
        "            total_words += 1\n",
        "\n",
        "    # Фильтрация редких слов и сортировка по частоте\n",
        "    filtered_words = [(word, count) for word, count in word_counts.items() if count >= min_word_count]\n",
        "    sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Заполнение словаря\n",
        "    for word, _ in sorted_words[:max_vocab_size - 4]:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    # Анализ покрытия\n",
        "    covered_words = sum(count for word, count in word_counts.items() if word in vocab)\n",
        "    coverage = covered_words / total_words * 100\n",
        "\n",
        "    print(f\"Словарь: {len(vocab)} слов\")\n",
        "    print(f\"Покрытие текста: {coverage:.2f}%\")\n",
        "    print(f\"Отброшено редких слов (<{min_word_count} вхождений): {len(word_counts) - len(filtered_words)}\")\n",
        "\n",
        "    return vocab\n",
        "\n",
        "vocab_en = build_vocab(full_data['en'], min_word_count=5)\n",
        "vocab_ru = build_vocab(full_data['ru'], min_word_count=3)"
      ],
      "metadata": {
        "id": "C8llwaI-66DQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a1f791-2fad-408b-a072-16b40d109607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь: 50000 слов\n",
            "Покрытие текста: 94.39%\n",
            "Отброшено редких слов (<5 вхождений): 1582755\n",
            "Словарь: 50000 слов\n",
            "Покрытие текста: 84.92%\n",
            "Отброшено редких слов (<3 вхождений): 2623982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"EN словарь: {len(vocab_en)} слов\")\n",
        "print(f\"RU словарь: {len(vocab_ru)} слов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k4bdXviT0FG",
        "outputId": "19ab7780-c8a3-468a-f487-8b9356d4bc4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN словарь: 50000 слов\n",
            "RU словарь: 50000 слов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(full_data, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "hwshhF2tlzWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. СОЗДАНИЕ DATALOADER\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, en_texts, ru_texts, vocab_en, vocab_ru, max_length=64):\n",
        "        self.en_texts = en_texts.tolist()\n",
        "        self.ru_texts = ru_texts.tolist()\n",
        "        self.vocab_en = vocab_en\n",
        "        self.vocab_ru = vocab_ru\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_text = self.en_texts[idx]\n",
        "        ru_text = self.ru_texts[idx]\n",
        "\n",
        "        # Токенизация EN\n",
        "        en_tokens = [self.vocab_en['<sos>']]\n",
        "        en_tokens += [self.vocab_en.get(word, self.vocab_en['<unk>'])\n",
        "                      for word in en_text.split()[:self.max_length-2]]\n",
        "        en_tokens.append(self.vocab_en['<eos>'])\n",
        "        en_tokens = en_tokens[:self.max_length]\n",
        "        en_tokens += [self.vocab_en['<pad>']] * (self.max_length - len(en_tokens))\n",
        "\n",
        "        # Токенизация RU\n",
        "        ru_tokens = [self.vocab_ru['<sos>']]\n",
        "        ru_tokens += [self.vocab_ru.get(word, self.vocab_ru['<unk>'])\n",
        "                      for word in ru_text.split()[:self.max_length-2]]\n",
        "        ru_tokens.append(self.vocab_ru['<eos>'])\n",
        "        ru_tokens = ru_tokens[:self.max_length]\n",
        "        ru_tokens += [self.vocab_ru['<pad>']] * (self.max_length - len(ru_tokens))\n",
        "\n",
        "        return (\n",
        "            torch.tensor(en_tokens, dtype=torch.long),\n",
        "            torch.tensor(ru_tokens, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "train_dataset = TranslationDataset(train_data['en'], train_data['ru'], vocab_en, vocab_ru)\n",
        "test_dataset = TranslationDataset(test_data['en'], test_data['ru'], vocab_en, vocab_ru)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    pin_memory=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    pin_memory=False,\n",
        "    num_workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "-m5mQT-T9Ale",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d1edc3-a45a-41cd-b700-959f8b021eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель --> 1. Определение архитектуры --> 2. Определение модели --> Тестирование модели"
      ],
      "metadata": {
        "id": "XML34jaDM9a3"
      }
    },
    {
      "source": [
        "# 4. МОДЕЛЬ\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.encoder_embed = nn.Embedding(input_vocab_size, hidden_size, padding_idx=0)\n",
        "        self.encoder_lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.decoder_embed = nn.Embedding(output_vocab_size, hidden_size, padding_idx=0)\n",
        "        self.decoder_lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # Энкодер\n",
        "        src_embedded = self.encoder_embed(src)\n",
        "        _, (hidden, cell) = self.encoder_lstm(src_embedded)\n",
        "\n",
        "        # Декодер\n",
        "        trg_embedded = self.decoder_embed(trg[:, :-1])  # Убираем последний токен\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(trg_embedded.size(1)):\n",
        "            lstm_out, (hidden, cell) = self.decoder_lstm(\n",
        "                trg_embedded[:, i].unsqueeze(1),\n",
        "                (hidden, cell)\n",
        "            )\n",
        "            outputs.append(self.fc(lstm_out.squeeze(1)))\n",
        "\n",
        "        return torch.stack(outputs, dim=1)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EJZ5ggygMURX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq(len(vocab_en), len(vocab_ru), 128)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DninuqHNntC",
        "outputId": "90fc2aba-6ff1-40cd-87aa-e9809d2e6b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (encoder_embed): Embedding(50000, 128, padding_idx=0)\n",
            "  (encoder_lstm): LSTM(128, 128, batch_first=True)\n",
            "  (decoder_embed): Embedding(50000, 128, padding_idx=0)\n",
            "  (decoder_lstm): LSTM(128, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=50000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение -- 1. Определение оптимайзера --> 2. Проверка вычислительных потерь"
      ],
      "metadata": {
        "id": "rXX_afAwO2ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ГРАФИКИ\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # График потерь\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # График точности (если есть)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot([1/x for x in train_losses], label='Train PPL')\n",
        "    plt.plot([1/x for x in val_losses], label='Val PPL')\n",
        "    plt.title('Perplexity')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('PPL')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "29EYo-m5vDCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ячейка загрузки чекпоинтов\n",
        "import glob\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, model, optimizer):\n",
        "    checkpoints = glob.glob(os.path.join(checkpoint_dir, \"epoch_*.pt\"))\n",
        "    if not checkpoints:\n",
        "        print(\"No checkpoints found. Starting from scratch.\")\n",
        "        return {\n",
        "            'start_epoch': 0,\n",
        "            'best_val_loss': float('inf'),\n",
        "            'best_epoch': 0,\n",
        "            'train_losses': [],\n",
        "            'val_losses': []\n",
        "        }\n",
        "\n",
        "    latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
        "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
        "    checkpoint = torch.load(latest_checkpoint)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Загружаем лучшую модель если существует\n",
        "    best_model_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
        "    if os.path.exists(best_model_path):\n",
        "        best_checkpoint = torch.load(best_model_path)\n",
        "        best_val_loss = best_checkpoint['val_loss']\n",
        "        best_epoch = best_checkpoint['epoch']\n",
        "    else:\n",
        "        best_val_loss = checkpoint['val_loss']\n",
        "        best_epoch = checkpoint['epoch']\n",
        "\n",
        "    return {\n",
        "        'start_epoch': checkpoint['epoch'],\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'best_epoch': best_epoch,\n",
        "        'train_losses': checkpoint.get('train_loss_history', []),\n",
        "        'val_losses': checkpoint.get('val_loss_history', [])\n",
        "    }\n",
        "\n",
        "# Использование:\n",
        "checkpoint_data = load_checkpoint(\"checkpoints\", model, optimizer)\n",
        "start_epoch = checkpoint_data['start_epoch']\n",
        "best_val_loss = checkpoint_data['best_val_loss']\n",
        "best_epoch = checkpoint_data['best_epoch']\n",
        "train_losses = checkpoint_data['train_losses']\n",
        "val_losses = checkpoint_data['val_losses']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfyVz5n8zyPK",
        "outputId": "7484acf1-4feb-40c0-9252-9f7324edcc61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoints found. Starting from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. ОБУЧЕНИЕ\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "checkpoint_dir = \"checkpoints\"  # ← Вот здесь определяется!\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Данные для визуализации\n",
        "train_losses, val_losses = [], []\n",
        "max_validation_batches = 500  # Ограничение батчей при валидации\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch + 3):  # Начинаем с start_epoch\n",
        "    # Фаза обучения\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    processed_batches = 0\n",
        "\n",
        "    train_pbar = tqdm(train_dataloader,\n",
        "                     desc=f\"Epoch {epoch+1}/{start_epoch+3} [Train]\",\n",
        "                     unit=\"batch\",\n",
        "                     bar_format=\"{l_bar}{bar:20}{r_bar}\")\n",
        "\n",
        "    for src, trg in train_pbar:\n",
        "      if processed_batches >= 1000:\n",
        "          train_pbar.close()\n",
        "          break\n",
        "\n",
        "      # Основной тренинг (ДОЛЖЕН ВЫПОЛНЯТЬСЯ ВСЕГДА)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(src, trg)\n",
        "      loss = criterion(\n",
        "          output.reshape(-1, output.size(-1)),\n",
        "          trg[:, 1:].contiguous().reshape(-1)\n",
        "      )\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      processed_batches += 1\n",
        "      train_pbar.set_postfix({\"train_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_train_loss = train_loss / processed_batches\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Фаза валидации\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    val_batches_processed = 0\n",
        "\n",
        "    val_pbar = tqdm(test_dataloader,\n",
        "                   desc=f\"Epoch {epoch+1}/{start_epoch+3} [Val]\",\n",
        "                   unit=\"batch\",\n",
        "                   bar_format=\"{l_bar}{bar:20}{r_bar}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in val_pbar:\n",
        "            if val_batches_processed >= max_validation_batches:\n",
        "                val_pbar.close()\n",
        "                break\n",
        "\n",
        "            output = model(src, trg)\n",
        "            loss = criterion(\n",
        "                output.reshape(-1, output.size(-1)),\n",
        "                trg[:, 1:].contiguous().reshape(-1)\n",
        "            )\n",
        "            test_loss += loss.item()\n",
        "            val_batches_processed += 1\n",
        "            val_pbar.set_postfix({\"val_loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_test_loss = test_loss / val_batches_processed\n",
        "    val_losses.append(avg_test_loss)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Results:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_test_loss:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Сохранение чекпоинта\n",
        "    if (epoch % 3 == 0):\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_test_loss,\n",
        "            'train_loss_history': train_losses,\n",
        "            'val_loss_history': val_losses,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    # Обновление лучшей модели\n",
        "    if avg_test_loss < best_val_loss:\n",
        "        best_val_loss = avg_test_loss\n",
        "        best_epoch = epoch + 1\n",
        "        best_model_path = os.path.join(checkpoint_dir, \"best_model.pt\")\n",
        "        torch.save({\n",
        "            'epoch': best_epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': best_val_loss,\n",
        "            'train_loss_history': train_losses,\n",
        "            'val_loss_history': val_losses,\n",
        "        }, best_model_path)\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "plot_training_history(train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI8gqGorZPWy",
        "outputId": "9ed06d37-b946-48b0-ec95-482bda41279b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/3 [Train]:   0%|                    | 0/703125 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/3 [Train]:   0%|                    | 1/703125 [00:09<1886:19:37,  9.66s/batch, train_loss=10.8138]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инференс -- Преобразование результата в слова"
      ],
      "metadata": {
        "id": "bh5KOkyFZXn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. ТЕСТИРОВАНИЕ\n",
        "def translate(model, sentence, vocab_en, vocab_ru, max_length=32):\n",
        "    model.eval()\n",
        "\n",
        "    # Подготовка входных токенов\n",
        "    tokens = [vocab_en['<sos>']]\n",
        "    tokens += [vocab_en.get(word.lower(), vocab_en['<unk>']) for word in str(sentence).split()]\n",
        "    tokens.append(vocab_en['<eos>'])\n",
        "    tokens = tokens[:max_length]\n",
        "    tokens += [vocab_en['<pad>']] * (max_length - len(tokens))\n",
        "\n",
        "    src = torch.LongTensor(tokens).unsqueeze(0)  # [1, seq_len]\n",
        "\n",
        "    # Энкодинг\n",
        "    with torch.no_grad():\n",
        "        src_embedded = model.encoder_embed(src)\n",
        "        _, (hidden, cell) = model.encoder_lstm(src_embedded)\n",
        "\n",
        "        # Декодинг\n",
        "        trg = torch.LongTensor([[vocab_ru['<sos>']]])  # [1, 1]\n",
        "        output_sentence = []\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            trg_embedded = model.decoder_embed(trg)\n",
        "            output, (hidden, cell) = model.decoder_lstm(trg_embedded, (hidden, cell))\n",
        "            output = model.fc(output.squeeze(1))\n",
        "            next_token = output.argmax(-1).item()\n",
        "\n",
        "            if next_token == vocab_ru['<eos>']:\n",
        "                break\n",
        "\n",
        "            output_sentence.append(next_token)\n",
        "            trg = torch.LongTensor([[next_token]])\n",
        "\n",
        "    # Преобразование индексов в слова\n",
        "    ru_vocab_rev = {v: k for k, v in vocab_ru.items()}\n",
        "    return ' '.join([ru_vocab_rev[idx] for idx in output_sentence])"
      ],
      "metadata": {
        "id": "8nr5HtnmwtX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_phrases = [\"hello world\", \"how are you\", \"I love machine learning\"]\n",
        "for phrase in test_phrases:\n",
        "    print(f\"'{phrase}' -> '{translate(model, phrase, vocab_en, vocab_ru)}'\")"
      ],
      "metadata": {
        "id": "tjRNSs8pkF3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Есть ли 'hello' в vocab_en?\", 'hello' in vocab_en)\n",
        "print(\"Есть ли 'world' в vocab_en?\", 'world' in vocab_en)\n",
        "print(\"Первые 20 слов vocab_en:\", list(vocab_en.keys())[:20])\n",
        "print(\"Первые 20 слов vocab_ru:\", list(vocab_ru.keys())[:20])"
      ],
      "metadata": {
        "id": "8MR_RzX5yD1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}